<!DOCTYPE HTML>
<html lang="cn" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Vincent教你学AI </title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="preface.html">Preface</a></li><li class="chapter-item expanded "><a href="diffusion.html"><strong aria-hidden="true">1.</strong> Diffusion Model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="diffusion/ddpm.html"><strong aria-hidden="true">1.1.</strong> DDPM</a></li><li class="chapter-item expanded "><a href="diffusion/stable-diffusion.html"><strong aria-hidden="true">1.2.</strong> Stable Diffusion</a></li><li class="chapter-item expanded "><a href="diffusion/consistency.html"><strong aria-hidden="true">1.3.</strong> Consistency Models</a></li></ol></li><li class="chapter-item expanded "><a href="normalizing-flow.html"><strong aria-hidden="true">2.</strong> Normalizing Flow</a></li><li class="chapter-item expanded "><a href="flow-matching.html"><strong aria-hidden="true">3.</strong> Flow Matching</a></li><li class="chapter-item expanded "><a href="vae.html"><strong aria-hidden="true">4.</strong> Variational Autoencoder (VAE)</a></li><li class="chapter-item expanded "><a href="gan.html"><strong aria-hidden="true">5.</strong> Generative Adversarial Network (GAN)</a></li><li class="chapter-item expanded "><a href="auto-regressive.html"><strong aria-hidden="true">6.</strong> Auto-regressive Model</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Vincent教你学AI </h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="preface"><a class="header" href="#preface">Preface</a></h1>
<p>Welcome to the AI Generative Models guide. This book serves as a comprehensive resource for understanding various generative AI models that have revolutionized the field of artificial intelligence in recent years.</p>
<h2 id="purpose-of-this-book"><a class="header" href="#purpose-of-this-book">Purpose of This Book</a></h2>
<p>This guide aims to provide a clear and concise overview of key generative AI models, with a focus on:</p>
<ul>
<li><strong>Diffusion Models</strong>: Including DDPM, Stable Diffusion, and Consistency Models</li>
<li><strong>Normalizing Flows</strong>: Models that use invertible transformations</li>
<li><strong>Flow Matching</strong>: Continuous-time flow-based generative models</li>
<li><strong>Variational Autoencoders (VAEs)</strong>: Latent variable models with probabilistic encoders and decoders</li>
<li><strong>Generative Adversarial Networks (GANs)</strong>: Adversarial framework with generator and discriminator networks</li>
<li><strong>Auto-regressive Models</strong>: Such as transformers used in large language models</li>
</ul>
<p>Each section explores the theoretical foundations, architectures, and practical applications of these models, making complex concepts accessible to readers with varying levels of technical expertise.</p>
<h2 id="how-to-use-this-guide"><a class="header" href="#how-to-use-this-guide">How to Use This Guide</a></h2>
<p>The book is structured to allow both sequential reading and targeted exploration of specific models. You can:</p>
<ol>
<li>Follow the chapters in order for a complete understanding of the landscape</li>
<li>Jump directly to specific model types that interest you</li>
<li>Use the reference sections to find original papers and additional resources</li>
</ol>
<p>We hope this guide enhances your understanding of generative AI and inspires your own experiments and applications in this exciting field.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diffusion-model"><a class="header" href="#diffusion-model">Diffusion Model</a></h1>
<p>Diffusion models are a class of generative models that have revolutionized AI-based content creation. These models work by gradually adding noise to data and then learning to reverse this process to generate new samples.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Diffusion models have emerged as one of the most powerful approaches for generative AI, particularly for images, but increasingly for other modalities like audio, video, and 3D content. They work by:</p>
<ol>
<li>Forward process: Gradually adding noise to data until it becomes pure noise</li>
<li>Reverse process: Learning to iteratively denoise to generate new data</li>
</ol>
<h2 id="key-advantages"><a class="header" href="#key-advantages">Key Advantages</a></h2>
<ul>
<li>High-quality generation</li>
<li>Flexible conditioning mechanisms</li>
<li>Stable training compared to GANs</li>
<li>Strong theoretical foundation</li>
</ul>
<h2 id="types-of-diffusion-models"><a class="header" href="#types-of-diffusion-models">Types of Diffusion Models</a></h2>
<p>This section covers several important variants of diffusion models:</p>
<ul>
<li><a href="./diffusion/ddpm.html">DDPM</a>: The original denoising diffusion probabilistic models</li>
<li><a href="./diffusion/stable-diffusion.html">Stable Diffusion</a>: Latent diffusion models for text-to-image generation</li>
<li><a href="./diffusion/consistency.html">Consistency Models</a>: Fast sampling diffusion variants</li>
</ul>
<p>Each sub-chapter provides detailed explanations of the model architecture, training process, and applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="denoising-diffusion-probabilistic-models-ddpm"><a class="header" href="#denoising-diffusion-probabilistic-models-ddpm">Denoising Diffusion Probabilistic Models (DDPM)</a></h1>
<p>Denoising Diffusion Probabilistic Models (DDPM) are a class of generative models that learn to generate data by reversing a gradual noising process.</p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<ul>
<li>Forward diffusion process: gradually adds Gaussian noise to data</li>
<li>Reverse diffusion process: learns to denoise images step by step</li>
<li>U-Net architecture: commonly used as the denoising network</li>
</ul>
<h2 id="mathematical-formulation"><a class="header" href="#mathematical-formulation">Mathematical Formulation</a></h2>
<p>DDPMs operate by defining a forward diffusion process that gradually adds noise to data until it becomes pure noise, and then training a model to reverse this process.</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2006.11239">Ho et al. (2020), "Denoising Diffusion Probabilistic Models"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stable-diffusion"><a class="header" href="#stable-diffusion">Stable Diffusion</a></h1>
<p>Stable Diffusion is a latent diffusion model developed by Stability AI that generates detailed images from text descriptions.</p>
<h2 id="architecture"><a class="header" href="#architecture">Architecture</a></h2>
<ul>
<li>Latent space diffusion: applies diffusion in a compressed latent space</li>
<li>Text conditioning: uses CLIP text embeddings to guide the generation</li>
<li>VAE: encodes images to latent space and decodes them back</li>
<li>UNet: performs the denoising in the latent space</li>
</ul>
<h2 id="key-features"><a class="header" href="#key-features">Key Features</a></h2>
<ul>
<li>Text-to-image generation</li>
<li>Image-to-image transformation</li>
<li>Inpainting and outpainting</li>
<li>Fine-tuning capabilities</li>
</ul>
<h2 id="applications"><a class="header" href="#applications">Applications</a></h2>
<ul>
<li>Creative art generation</li>
<li>Design prototyping</li>
<li>Content creation</li>
<li>Image editing</li>
</ul>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2112.10752">Rombach et al. (2022), "High-Resolution Image Synthesis with Latent Diffusion Models"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="consistency-models"><a class="header" href="#consistency-models">Consistency Models</a></h1>
<p>Consistency Models are a type of generative model that offers faster sampling than traditional diffusion models while maintaining generation quality.</p>
<h2 id="key-innovations"><a class="header" href="#key-innovations">Key Innovations</a></h2>
<ul>
<li>Single-step generation: can generate high-quality samples in just one step</li>
<li>Few-step generation: offers quality-speed trade-off with few sampling steps</li>
<li>Distillation approach: distills knowledge from a pre-trained diffusion model</li>
</ul>
<h2 id="advantages"><a class="header" href="#advantages">Advantages</a></h2>
<ul>
<li>Much faster sampling than regular diffusion models</li>
<li>Deterministic sampling process</li>
<li>Maintains generation quality comparable to diffusion models</li>
</ul>
<h2 id="technical-details"><a class="header" href="#technical-details">Technical Details</a></h2>
<ul>
<li>Consistency functions: maps noise to data points</li>
<li>Consistency distillation: training technique to learn consistency functions</li>
<li>Self-conditioning: improves generation quality</li>
</ul>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2303.01469">Song et al. (2023), "Consistency Models"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="normalizing-flow"><a class="header" href="#normalizing-flow">Normalizing Flow</a></h1>
<p>Normalizing Flows are a family of generative models that use invertible transformations to map between a simple base distribution and a complex target distribution.</p>
<h2 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key Concepts</a></h2>
<ul>
<li><strong>Invertible Transformations</strong>: Bijective mappings between spaces</li>
<li><strong>Change of Variables Formula</strong>: Mathematical foundation that allows exact likelihood computation</li>
<li><strong>Flow-based Generation</strong>: Sampling from a simple distribution and transforming through learned flows</li>
</ul>
<h2 id="types-of-normalizing-flows"><a class="header" href="#types-of-normalizing-flows">Types of Normalizing Flows</a></h2>
<ul>
<li><strong>NICE/RealNVP</strong>: Coupling layers with affine transformations</li>
<li><strong>Glow</strong>: Extended RealNVP with 1x1 convolutions</li>
<li><strong>Autoregressive Flows (IAF, MAF)</strong>: Using autoregressive transformations</li>
<li><strong>Continuous Normalizing Flows</strong>: Defining flows using ordinary differential equations</li>
</ul>
<h2 id="advantages-1"><a class="header" href="#advantages-1">Advantages</a></h2>
<ul>
<li><strong>Exact Likelihood</strong>: Unlike VAEs, flows provide exact likelihood computation</li>
<li><strong>Efficient Sampling</strong>: Unlike autoregressive models, sampling can be parallelized</li>
<li><strong>Invertibility</strong>: Can transform in both directions (generation and inference)</li>
<li><strong>Stable Training</strong>: More stable than GANs, using maximum likelihood</li>
</ul>
<h2 id="applications-1"><a class="header" href="#applications-1">Applications</a></h2>
<ul>
<li><strong>Image Generation</strong>: High-quality image synthesis</li>
<li><strong>Anomaly Detection</strong>: Identifying outliers in data</li>
<li><strong>Density Estimation</strong>: Learning complex probability distributions</li>
<li><strong>Variational Inference</strong>: More expressive posterior approximations</li>
</ul>
<h2 id="challenges"><a class="header" href="#challenges">Challenges</a></h2>
<ul>
<li><strong>Architectural Constraints</strong>: Requiring invertibility limits model expressiveness</li>
<li><strong>Computational Cost</strong>: Some flows can be computationally expensive</li>
<li><strong>High-dimensional Data</strong>: Scaling to very high dimensions can be challenging</li>
</ul>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1505.05770">Rezende &amp; Mohamed (2015), "Variational Inference with Normalizing Flows"</a></li>
<li><a href="https://arxiv.org/abs/1605.08803">Dinh et al. (2016), "Density estimation using Real NVP"</a></li>
<li><a href="https://arxiv.org/abs/1807.03039">Kingma &amp; Dhariwal (2018), "Glow: Generative Flow with Invertible 1x1 Convolutions"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="flow-matching"><a class="header" href="#flow-matching">Flow Matching</a></h1>
<p>Flow Matching is a generative modeling technique that builds upon the ideas of continuous normalizing flows and probability flow ODEs, offering an alternative training approach for generative models.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>Flow Matching defines a continuous-time transformation from a simple distribution (like a Gaussian) to a complex target distribution. Unlike traditional normalizing flows, flow matching doesn't require computing the Jacobian determinant, making it more flexible and computationally efficient.</p>
<h2 id="key-concepts-2"><a class="header" href="#key-concepts-2">Key Concepts</a></h2>
<ul>
<li><strong>Vector Fields</strong>: Learning a vector field to represent the continuous-time flow</li>
<li><strong>Straight-line Paths</strong>: Using simple paths between distributions</li>
<li><strong>Conditional Flow Matching</strong>: Extending to conditional generation</li>
<li><strong>ODE-based Generation</strong>: Sampling by solving an ordinary differential equation</li>
</ul>
<h2 id="relation-to-other-models"><a class="header" href="#relation-to-other-models">Relation to Other Models</a></h2>
<ul>
<li><strong>Diffusion Models</strong>: Flow matching can be seen as a generalization of score-based diffusion models</li>
<li><strong>Normalizing Flows</strong>: Similar concept but with different training objectives</li>
<li><strong>Optimal Transport</strong>: Connection to optimal transport theory</li>
</ul>
<h2 id="advantages-2"><a class="header" href="#advantages-2">Advantages</a></h2>
<ul>
<li><strong>Flexible Architecture</strong>: No invertibility requirement</li>
<li><strong>Efficient Training</strong>: More efficient than score matching in some cases</li>
<li><strong>Theoretical Guarantees</strong>: Well-founded mathematical framework</li>
<li><strong>Stable Training</strong>: Often more stable than adversarial approaches</li>
</ul>
<h2 id="applications-2"><a class="header" href="#applications-2">Applications</a></h2>
<ul>
<li><strong>Image Generation</strong>: Creating realistic images</li>
<li><strong>Shape Generation</strong>: 3D shape synthesis</li>
<li><strong>Audio Synthesis</strong>: Generating audio waveforms</li>
<li><strong>Density Estimation</strong>: Learning complex distributions</li>
</ul>
<h2 id="references-4"><a class="header" href="#references-4">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/2210.02747">Lipman et al. (2022), "Flow Matching for Generative Modeling"</a></li>
<li><a href="https://arxiv.org/abs/2209.03003">Liu et al. (2022), "Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow"</a></li>
<li><a href="https://arxiv.org/abs/2302.00482">Tong et al. (2023), "Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="variational-autoencoder-vae"><a class="header" href="#variational-autoencoder-vae">Variational Autoencoder (VAE)</a></h1>
<p>Variational Autoencoders (VAEs) are a class of generative models that combine elements of neural networks with variational inference to learn latent representations of data.</p>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<p>VAEs consist of two main components:</p>
<ul>
<li><strong>Encoder Network</strong>: Maps input data to a distribution in latent space</li>
<li><strong>Decoder Network</strong>: Maps points from the latent space back to the data space</li>
</ul>
<h2 id="mathematical-framework"><a class="header" href="#mathematical-framework">Mathematical Framework</a></h2>
<ul>
<li><strong>Variational Inference</strong>: Approximating intractable posterior distributions</li>
<li><strong>Evidence Lower Bound (ELBO)</strong>: Objective function combining reconstruction and regularization</li>
<li><strong>Reparameterization Trick</strong>: Enabling backpropagation through the sampling process</li>
</ul>
<h2 id="key-variants"><a class="header" href="#key-variants">Key Variants</a></h2>
<ul>
<li><strong>β-VAE</strong>: Introduces a weighting factor for the KL divergence term</li>
<li><strong>Conditional VAE</strong>: Incorporates conditional information into generation</li>
<li><strong>VQ-VAE</strong>: Uses vector quantization in the latent space</li>
<li><strong>Hierarchical VAE</strong>: Employs multiple levels of latent variables</li>
</ul>
<h2 id="advantages-3"><a class="header" href="#advantages-3">Advantages</a></h2>
<ul>
<li><strong>Structured Latent Space</strong>: Creates a continuous and meaningful latent space</li>
<li><strong>Generative Capabilities</strong>: Can generate new samples from the learned distribution</li>
<li><strong>Unsupervised Learning</strong>: Doesn't require labeled data</li>
<li><strong>Probabilistic Framework</strong>: Provides uncertainty estimates</li>
</ul>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<ul>
<li><strong>Blurry Outputs</strong>: Often produces blurrier results than GANs</li>
<li><strong>Posterior Collapse</strong>: Can ignore some latent variables</li>
<li><strong>Approximation Gap</strong>: The true and approximate posterior may differ significantly</li>
</ul>
<h2 id="applications-3"><a class="header" href="#applications-3">Applications</a></h2>
<ul>
<li><strong>Image Generation</strong>: Creating new realistic images</li>
<li><strong>Representation Learning</strong>: Finding meaningful data representations</li>
<li><strong>Anomaly Detection</strong>: Identifying unusual patterns</li>
<li><strong>Data Compression</strong>: Efficient encoding of complex data</li>
</ul>
<h2 id="references-5"><a class="header" href="#references-5">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1312.6114">Kingma &amp; Welling (2014), "Auto-Encoding Variational Bayes"</a></li>
<li><a href="https://arxiv.org/abs/1401.4082">Rezende et al. (2014), "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"</a></li>
<li><a href="https://openreview.net/pdf?id=Sy2fzU9gl">Higgins et al. (2017), "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generative-adversarial-network-gan"><a class="header" href="#generative-adversarial-network-gan">Generative Adversarial Network (GAN)</a></h1>
<p>Generative Adversarial Networks (GANs) are a framework for training generative models through an adversarial process involving two neural networks - a generator and a discriminator.</p>
<h2 id="core-architecture"><a class="header" href="#core-architecture">Core Architecture</a></h2>
<ul>
<li><strong>Generator Network</strong>: Creates synthetic data samples from random noise</li>
<li><strong>Discriminator Network</strong>: Attempts to distinguish between real and generated samples</li>
<li><strong>Adversarial Training</strong>: The two networks are trained in a competitive minimax game</li>
</ul>
<h2 id="mathematical-framework-1"><a class="header" href="#mathematical-framework-1">Mathematical Framework</a></h2>
<ul>
<li><strong>Minimax Game</strong>: Formulation as a two-player zero-sum game</li>
<li><strong>Nash Equilibrium</strong>: Theoretical solution point where neither network can improve</li>
<li><strong>Non-saturating Loss</strong>: Modified objective to prevent generator gradient vanishing</li>
</ul>
<h2 id="key-gan-variants"><a class="header" href="#key-gan-variants">Key GAN Variants</a></h2>
<ul>
<li><strong>DCGAN</strong>: Introduces convolutional architectures for stable training</li>
<li><strong>Conditional GAN</strong>: Incorporates class or attribute information</li>
<li><strong>CycleGAN</strong>: Enables unpaired image-to-image translation</li>
<li><strong>StyleGAN</strong>: Progressive growing with style-based generator for high-quality images</li>
<li><strong>BigGAN</strong>: Scaled-up architecture for high-fidelity image generation</li>
<li><strong>WGAN</strong>: Wasserstein GAN with improved training stability</li>
</ul>
<h2 id="advantages-4"><a class="header" href="#advantages-4">Advantages</a></h2>
<ul>
<li><strong>Sharp, Realistic Outputs</strong>: Produces crisp, high-quality samples</li>
<li><strong>Implicit Density Modeling</strong>: No need to explicitly define a likelihood function</li>
<li><strong>Versatile Framework</strong>: Applicable to many data types and problems</li>
<li><strong>Strong Empirical Results</strong>: State-of-the-art in many image generation tasks</li>
</ul>
<h2 id="challenges-1"><a class="header" href="#challenges-1">Challenges</a></h2>
<ul>
<li><strong>Training Instability</strong>: Prone to mode collapse and oscillating behavior</li>
<li><strong>Evaluation Difficulty</strong>: Hard to quantitatively evaluate quality</li>
<li><strong>Lack of Inference</strong>: Standard GANs don't provide inference capabilities</li>
<li><strong>Requires Careful Tuning</strong>: Sensitive to hyperparameters and architecture choices</li>
</ul>
<h2 id="applications-4"><a class="header" href="#applications-4">Applications</a></h2>
<ul>
<li><strong>Image Generation</strong>: Creating photorealistic images</li>
<li><strong>Image Translation</strong>: Converting between image domains</li>
<li><strong>Data Augmentation</strong>: Generating synthetic training data</li>
<li><strong>Super-resolution</strong>: Enhancing low-resolution images</li>
<li><strong>Art Generation</strong>: Creating novel artistic content</li>
</ul>
<h2 id="references-6"><a class="header" href="#references-6">References</a></h2>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661">Goodfellow et al. (2014), "Generative Adversarial Networks"</a></li>
<li><a href="https://arxiv.org/abs/1511.06434">Radford et al. (2015), "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"</a></li>
<li><a href="https://arxiv.org/abs/1812.04948">Karras et al. (2019), "A Style-Based Generator Architecture for Generative Adversarial Networks"</a></li>
<li><a href="https://arxiv.org/abs/1701.07875">Arjovsky et al. (2017), "Wasserstein GAN"</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="auto-regressive-model"><a class="header" href="#auto-regressive-model">Auto-regressive Model</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
