1
00:00:00,100 --> 00:00:00,333
大家好

2
00:00:00,333 --> 00:00:00,633
今天给大家介绍一个

3
00:00:01,700 --> 00:00:04,333
关于diffusion model里面的VAE的优化的

4
00:00:04,333 --> 00:00:05,900
一个非常简单的一个技巧

5
00:00:06,333 --> 00:00:07,500
这个故事的开端呢

6
00:00:07,500 --> 00:00:08,133
是一个叫even呢

7
00:00:08,666 --> 00:00:10,300
snap的公司的一个大佬

8
00:00:10,300 --> 00:00:12,966
在推特上分享了一个帖子

9
00:00:13,600 --> 00:00:16,300
是说他们的一个idea和另外一个paper

10
00:00:16,300 --> 00:00:17,400
在短短的1.5周

11
00:00:18,866 --> 00:00:20,533
大概10天之内就冲突了

12
00:00:21,366 --> 00:00:21,800
这个

13
00:00:21,800 --> 00:00:24,400
正因为这个idea本身的话非常simple

14
00:00:24,400 --> 00:00:26,666
所以在网上引起了很大的关注

15
00:00:27,300 --> 00:00:27,966
这个idea呢

16
00:00:27,966 --> 00:00:29,366
其实你看这个自然段

17
00:00:29,366 --> 00:00:30,500
就可以大概get到

18
00:00:30,500 --> 00:00:30,733
它想表达什么样的意思呢

19
00:00:32,800 --> 00:00:34,166
大概意思就是说

20
00:00:34,166 --> 00:00:35,966
通过在你的这个Pixel的space

21
00:00:37,666 --> 00:00:37,966
和VAE的Latent space上做scaling

22
00:00:40,566 --> 00:00:41,666
scaling化是指

23
00:00:41,666 --> 00:00:42,566
你比如说

24
00:00:42,566 --> 00:00:43,100
通过Linear或者cubic Linear的这种up-sampling

25
00:00:46,700 --> 00:00:47,900
或者down sampling

26
00:00:48,333 --> 00:00:50,000
你希望他们在latent space

27
00:00:50,733 --> 00:00:52,600
和pixel space上做的scaling

28
00:00:52,600 --> 00:00:54,300
以后都有相同的效果

29
00:00:54,566 --> 00:00:56,366
通过一个非常simple的regularization loss

30
00:00:56,966 --> 00:00:57,900
来约束他们

31
00:00:57,900 --> 00:00:59,266
他们可以说可以和很强的baseline

32
00:01:01,333 --> 00:01:03,600
比如说DiT或者REPA的baseline

33
00:01:03,600 --> 00:01:04,533
上面可以得到4倍的7倍的收敛速度

34
00:01:06,666 --> 00:01:07,833
训练的收敛速度

35
00:01:07,866 --> 00:01:09,200
那么这就是这个idea本身

36
00:01:09,200 --> 00:01:10,866
如果你不想了解细节的话

37
00:01:10,866 --> 00:01:11,566
听到这里的话

38
00:01:11,566 --> 00:01:13,300
其实后面可以不用听了

39
00:01:13,933 --> 00:01:14,800
但是呢，为了让大家深入理解

40
00:01:15,866 --> 00:01:17,966
这个作者是怎么分析这个方法的

41
00:01:17,966 --> 00:01:18,833
所以后面我会介绍

42
00:01:19,533 --> 00:01:21,700
重点介绍VQ-VAE这篇paper

43
00:01:21,700 --> 00:01:25,133
另外一篇paper也是这个作者(Ivan)自己写的

44
00:01:25,266 --> 00:01:26,466
但是我觉得他写的

45
00:01:26,466 --> 00:01:28,133
没有这个VQ-VAE这个方paper写的容易懂

46
00:01:29,900 --> 00:01:30,466
所以这里我们

47
00:01:30,700 --> 00:01:31,766
就直接从EQ-VAE的paper开始介绍

48
00:01:33,966 --> 00:01:34,366
首先需要说明一下

49
00:01:35,866 --> 00:01:37,166
其实现在作为设计方面

50
00:01:38,466 --> 00:01:41,333
大部分的工作都在优化

51
00:01:41,400 --> 00:01:43,600
stable diffusion框架里面的latent model

52
00:01:43,600 --> 00:01:44,266
但是针对于这个VAE架构的

53
00:01:46,200 --> 00:01:47,300
直接的优化的话

54
00:01:47,300 --> 00:01:47,966
其实相对而言工作是比较少的

55
00:01:49,733 --> 00:01:50,433
所以这边工作其实还是比较可贵的

56
00:01:52,233 --> 00:01:54,566
我们可以看到在作者的figure 1里面

57
00:01:54,566 --> 00:01:56,000
作者主要是claim他们

58
00:01:56,366 --> 00:01:57,966
有两个优势

59
00:01:58,400 --> 00:01:59,700
第一个优势就是说

60
00:01:59,700 --> 00:02:01,733
和传统的这种stable diffusion的feature

61
00:02:01,733 --> 00:02:02,700
比如说SD VAE的这种feature

62
00:02:04,166 --> 00:02:05,333
和做的自己的feature比起来的话

63
00:02:06,266 --> 00:02:07,500
他的feature会更加的干净

64
00:02:07,933 --> 00:02:11,066
第二个是想说他和一些很强的baseline

65
00:02:11,066 --> 00:02:12,333
比如说DIT或者REPA

66
00:02:12,800 --> 00:02:14,400
更强的baseline对比起来的话

67
00:02:14,566 --> 00:02:17,766
他有很高的训练的上的收敛速度

68
00:02:18,100 --> 00:02:20,066
达到4倍到7倍的收敛速度

69
00:02:20,133 --> 00:02:21,766
作者首先在方法部分的话

70
00:02:22,866 --> 00:02:24,466
就是介绍了一下

71
00:02:24,466 --> 00:02:26,333
传统的在stable diffusion里面

72
00:02:26,333 --> 00:02:26,866
这个训练 VAE的时候

73
00:02:28,366 --> 00:02:29,200
loss大概是由哪几部分组成的

74
00:02:31,000 --> 00:02:31,300
这里主要是有三部分

75
00:02:32,533 --> 00:02:34,800
第一部分是reconstruction loss

76
00:02:35,133 --> 00:02:38,300
第第二部分基于GAN的一个perceptual loss

77
00:02:38,300 --> 00:02:39,000
这个loss的话

78
00:02:39,000 --> 00:02:41,166
对于提升你的FID的质量的话

79
00:02:41,166 --> 00:02:42,266
是非常重要的

80
00:02:42,300 --> 00:02:43,200
第三部分的话

81
00:02:43,200 --> 00:02:45,266
是基于这个KL Divergence的

82
00:02:45,266 --> 00:02:46,933
一个regularization的loss

83
00:02:47,800 --> 00:02:49,533
那么这是基本的background

84
00:02:49,600 --> 00:02:51,766
那我们先来看figure two

85
00:02:51,866 --> 00:02:52,666
figure two里面

86
00:02:52,666 --> 00:02:54,433
这个其实想表达意思非常简单

87
00:02:55,033 --> 00:02:55,666
比如说

88
00:02:55,666 --> 00:02:58,200
如果直接用stable diffusion VAE去在pixel space

89
00:02:59,766 --> 00:03:00,566
或者data

90
00:03:01,600 --> 00:03:02,533
或者feature space

91
00:03:02,533 --> 00:03:04,166
做这种scaling的话

92
00:03:04,500 --> 00:03:05,500
你可以看到

93
00:03:05,500 --> 00:03:06,133
效果会跟原始图片比起来

94
00:03:07,300 --> 00:03:08,733
有比较大的偏差

95
00:03:09,100 --> 00:03:09,533
但是

96
00:03:09,533 --> 00:03:12,533
如果直接用作者训好了这个方法去做

97
00:03:12,566 --> 00:03:14,133
在latent space上面做这种scaling的话

98
00:03:15,200 --> 00:03:15,866
它的效果

99
00:03:15,866 --> 00:03:16,600
reconstruct重建的效果的话

100
00:03:17,966 --> 00:03:18,466
会比Stable Diffusion的VAE的效果要好

101
00:03:20,866 --> 00:03:22,166
在FIG3里面的话

102
00:03:22,166 --> 00:03:24,766
作者的对比就直接更加简单粗暴了

103
00:03:24,766 --> 00:03:25,800
作者在不同的

104
00:03:25,800 --> 00:03:27,266
比如说Stable Diffusion

105
00:03:27,266 --> 00:03:29,866
或者Stable Diffusion XL的VAE上面作为对照

106
00:03:30,800 --> 00:03:31,300
然后在这些VAE上面

107
00:03:32,766 --> 00:03:34,133
加上作者的这个trick

108
00:03:34,133 --> 00:03:34,800
以后

109
00:03:34,800 --> 00:03:35,500
可以发现他们训练好以后的VAE的话

110
00:03:37,466 --> 00:03:38,333
对于这个

111
00:03:38,700 --> 00:03:40,100
不同的scale的话

112
00:03:40,333 --> 00:03:41,433
可以达到更好的reconstruction RFID

113
00:03:43,466 --> 00:03:44,600
在实验部分呢

114
00:03:44,600 --> 00:03:46,600
作者针对于DIT

115
00:03:46,600 --> 00:03:46,866
SD或者REPA这些比较经典的方法

116
00:03:49,333 --> 00:03:50,100
作为它的baseline

117
00:03:51,000 --> 00:03:53,166
同时呢，作者不仅仅在这种continuous-state

118
00:03:53,900 --> 00:03:55,766
 model上面做了比较

119
00:03:55,766 --> 00:03:56,466
也在像MaskGiT

120
00:03:56,900 --> 00:03:57,600
这种discrete-state model上面

121
00:03:59,100 --> 00:04:01,000
也做了一些相应的比较

122
00:04:01,000 --> 00:04:03,066
作者这里的evaluation metric的话

123
00:04:03,066 --> 00:04:05,400
是比较使用非常经典的一些metric

124
00:04:05,766 --> 00:04:08,133
比如说FID,sFID

125
00:04:08,866 --> 00:04:09,966
Inception Score

126
00:04:10,100 --> 00:04:10,566
同时

127
00:04:10,566 --> 00:04:11,433
为了评价模型对于重建能力的好坏

128
00:04:14,066 --> 00:04:15,300
作者也使用了一些

129
00:04:15,300 --> 00:04:16,933
比如说经典的PSNR, SSIM

130
00:04:18,066 --> 00:04:18,533
包括LPIPS

131
00:04:19,166 --> 00:04:20,933
最后呢

132
00:04:20,933 --> 00:04:23,200
因为关于FID本身的话

133
00:04:23,200 --> 00:04:24,700
你可以分为是做重建的FID

134
00:04:26,066 --> 00:04:28,300
或者是做generation FID

135
00:04:28,300 --> 00:04:29,366
重建的FID的话

136
00:04:29,366 --> 00:04:31,000
相当于是说

137
00:04:31,000 --> 00:04:33,100
你整个模型FID的一个下限

138
00:04:33,766 --> 00:04:34,666
就说你不管你的diffusion

139
00:04:35,933 --> 00:04:37,366
model训训练多好

140
00:04:37,366 --> 00:04:38,900
你你所要达到的下限

141
00:04:38,900 --> 00:04:40,500
就是reconstruction最终达到

142
00:04:40,500 --> 00:04:41,300
那个FID

143
00:04:41,333 --> 00:04:43,966
所以这里作者会区分gFID或者rFID

144
00:04:44,900 --> 00:04:46,266
首先我们来看table one

145
00:04:47,466 --> 00:04:48,000
作者这里就对于一些比较

146
00:04:49,933 --> 00:04:51,333
比如说经典的一些结构

147
00:04:51,333 --> 00:04:52,933
比如说连续的结构(Continuous)

148
00:04:53,300 --> 00:04:54,366
stable diffusion的VAE

149
00:04:54,400 --> 00:04:56,133
Stable Diffusion XL的VAE

150
00:04:56,133 --> 00:04:57,933
Stable Diffusion 3的VAE

151
00:04:58,533 --> 00:05:01,300
包括这个discrete的这种VQ-GAN

152
00:05:01,300 --> 00:05:02,966
auto-encoder上面也做了对比

153
00:05:03,633 --> 00:05:04,800
我觉得大概conclusion说

154
00:05:05,300 --> 00:05:07,266
你的这个rFID会有稍微的下降

155
00:05:09,000 --> 00:05:11,266
但是你的这种generation

156
00:05:11,266 --> 00:05:12,833
RFID的话会极大程度的提升

157
00:05:14,400 --> 00:05:16,366
这是一个总的结果

158
00:05:16,666 --> 00:05:18,266
接下来我们再看table two

159
00:05:18,266 --> 00:05:18,866
table two的话

160
00:05:18,866 --> 00:05:19,966
主要是对这种generation FID做一些比较

161
00:05:21,766 --> 00:05:22,666
作者在关于DIT的这个架构

162
00:05:24,300 --> 00:05:24,733
还有SiT的这种架构

163
00:05:25,666 --> 00:05:26,900
在不同的参数量和迭代次数的情况下

164
00:05:28,333 --> 00:05:29,133
做了对比

165
00:05:29,200 --> 00:05:30,266
在所有情况下面

166
00:05:30,266 --> 00:05:32,766
都会比所有的比他们的相应的倍数量

167
00:05:32,766 --> 00:05:33,666
都会有比较大的提升

168
00:05:35,366 --> 00:05:36,500
在Table 3里面的话

169
00:05:36,500 --> 00:05:38,166
作者主要就是在discrete-state的

170
00:05:39,000 --> 00:05:41,066
这种model上面去做比较

171
00:05:41,066 --> 00:05:42,800
主要是和MaskGiT相比

172
00:05:42,966 --> 00:05:43,800
可以发现的话是说在达到相同的

173
00:05:46,366 --> 00:05:48,900
gFID的的情况下面，他们

174
00:05:49,200 --> 00:05:50,766
所需要的epoch的话

175
00:05:50,900 --> 00:05:53,133
只需要基本上一半的epoch

176
00:05:53,866 --> 00:05:55,066
如果进一步训练的话

177
00:05:55,066 --> 00:05:57,066
它可以达到更好的GFID

178
00:05:57,500 --> 00:05:58,300
在

179
00:05:58,400 --> 00:05:59,266
图4里面的话

180
00:05:59,266 --> 00:06:00,433
作者就想说

181
00:06:00,500 --> 00:06:03,333
不同的这种DIT-size的话

182
00:06:03,333 --> 00:06:04,033
他们的加了他们trick以后的话

183
00:06:05,600 --> 00:06:07,066
甚至效果在不同的any interaction上面

184
00:06:08,266 --> 00:06:09,500
效果会越来越好

185
00:06:09,500 --> 00:06:09,966
在table-four里面的话

186
00:06:10,933 --> 00:06:12,566
作者主要就是这一个

187
00:06:12,566 --> 00:06:13,900
关于这个任务

188
00:06:13,900 --> 00:06:14,266
非常经典的一

189
00:06:14,866 --> 00:06:16,000
个Benchmark

190
00:06:16,000 --> 00:06:17,066
比如说ImageNet256

191
00:06:17,066 --> 00:06:18,700
上面去做了比较

192
00:06:19,566 --> 00:06:21,733
首先就是在和一个基本的Baseline

193
00:06:21,733 --> 00:06:23,733
DIT-XL上面去做比较的话

194
00:06:23,733 --> 00:06:24,300
可以发现如果达到类似的FID的话

195
00:06:26,266 --> 00:06:28,000
它需要的这个epoch number

196
00:06:28,000 --> 00:06:29,933
会显著性的小很多

197
00:06:30,666 --> 00:06:31,900
做着和一个更强的REPA

198
00:06:32,566 --> 00:06:32,933
REPA应该是ICLR 2025

199
00:06:33,933 --> 00:06:34,966
今年的ICLR paper做比较的话

200
00:06:35,566 --> 00:06:36,300
可以发现

201
00:06:36,300 --> 00:06:37,566
达到类似的效果的话

202
00:06:37,566 --> 00:06:39,066
他需要的Epoch的话

203
00:06:39,066 --> 00:06:40,833
基本上只需要1/4

204
00:06:40,866 --> 00:06:43,300
但是作者这里好像没有显示

205
00:06:43,566 --> 00:06:44,866
如果继续Train下去

206
00:06:44,866 --> 00:06:46,733
比如说训练800个Epoch

207
00:06:46,733 --> 00:06:51,166
FID能够能不能够低于这个1.42

208
00:06:51,266 --> 00:06:52,466
在table 5里面的话

209
00:06:52,466 --> 00:06:52,933
作者就详细地

210
00:06:53,366 --> 00:06:55,333
对不同的spatial的transformation

211
00:06:55,333 --> 00:06:56,100
做了一个Ablation Study

212
00:06:57,266 --> 00:06:59,166
作者这里主要考虑了rotation和Scaling

213
00:07:00,766 --> 00:07:03,033
它主要想说的意思就是说

214
00:07:03,100 --> 00:07:05,166
Scaling带来的performance GAIN

215
00:07:05,166 --> 00:07:05,400
会比rotation大很多

216
00:07:07,400 --> 00:07:08,466
figure five的话

217
00:07:08,466 --> 00:07:10,366
这个话就对于大家来说

218
00:07:10,366 --> 00:07:11,666
是一个很好的福音了

219
00:07:11,666 --> 00:07:12,100
就是说

220
00:07:12,100 --> 00:07:15,966
你直接在这个Stable Diffusion的VAE上面

221
00:07:15,966 --> 00:07:16,866
去做一个fine-tuning的话

222
00:07:17,800 --> 00:07:18,733
就是说你只需要fine-tune

223
00:07:19,200 --> 00:07:22,033
用这他们设计的这个EQ-VAE的这个loss

224
00:07:22,033 --> 00:07:22,500
fine-tune

225
00:07:22,500 --> 00:07:23,233
五个epoch

226
00:07:23,333 --> 00:07:23,366
你

227
00:07:23,366 --> 00:07:24,600
你的这个gFID会显著性的下降

228
00:07:26,566 --> 00:07:28,733
最后我们再看一下appendix

229
00:07:28,733 --> 00:07:31,133
appendix里面

230
00:07:31,133 --> 00:07:32,533
因为如果你还记得前面

231
00:07:32,533 --> 00:07:33,533
作者前面介绍了

232
00:07:33,533 --> 00:07:35,466
两种不同的这种regularization

233
00:07:36,200 --> 00:07:38,100
一种是Explicit

234
00:07:38,100 --> 00:07:39,566
一种是Implicit

235
00:07:39,566 --> 00:07:41,000
作者主要是想说

236
00:07:41,133 --> 00:07:43,533
implicit的话是最管用的

237
00:07:43,533 --> 00:07:45,566
好啦，关于这篇工作

238
00:07:45,566 --> 00:07:46,266
就给大家介绍到这里了

239
00:07:48,000 --> 00:07:50,700
接下来就是带大家随便过一下

240
00:07:50,700 --> 00:07:51,400
这个做的Ivan自己的paper

241
00:07:53,000 --> 00:07:53,666
Ivan的话

242
00:07:53,666 --> 00:07:54,400
它的话和前面EQ-VAE的paper的话

243
00:07:56,600 --> 00:07:58,366
也是在强调同一个点

244
00:07:59,100 --> 00:07:59,900
就是说

245
00:07:59,933 --> 00:08:01,333
它在一个标准的一个Benchmark上面

246
00:08:02,533 --> 00:08:03,300
去券作者

247
00:08:03,300 --> 00:08:03,733
这里面主要是应该是一个image和video

248
00:08:06,266 --> 00:08:07,266
都包括了

249
00:08:07,300 --> 00:08:08,100
比较

250
00:08:08,233 --> 00:08:08,733
它们可以获得更快的

251
00:08:09,700 --> 00:08:11,500
这个确定的这种convergence收敛速度

252
00:08:12,266 --> 00:08:13,266
这篇文章大家可以看到

253
00:08:14,133 --> 00:08:16,700
其实这个方法本身的话

254
00:08:16,900 --> 00:08:19,166
在这个paper里面只有一个自然段

255
00:08:19,733 --> 00:08:20,866
大概意思就是说

256
00:08:20,866 --> 00:08:21,966
我们在这个input x and representation z上面

257
00:08:24,466 --> 00:08:26,100
都会做一些rescaling

258
00:08:26,933 --> 00:08:27,366
或者这个

259
00:08:27,366 --> 00:08:28,700
这个scaling的话是

260
00:08:28,700 --> 00:08:31,866
比如说是2到四倍的bilinear down-sampling

261
00:08:31,866 --> 00:08:33,566
然后就可以设计一个

262
00:08:33,566 --> 00:08:35,133
类似于这样的一个loss

263
00:08:35,133 --> 00:08:36,366
可以发现和前面VQ-VAE的loss的话

264
00:08:37,400 --> 00:08:38,466
是非常接近的

265
00:08:38,800 --> 00:08:39,900
尽管方法本身是差不多一样的

266
00:08:41,766 --> 00:08:42,566
但是Ivan的这个paper的话

267
00:08:43,600 --> 00:08:45,866
因为他们是在公司里面做的

268
00:08:45,866 --> 00:08:46,666
所以他们做的实验更加扎实

269
00:08:48,533 --> 00:08:51,133
比如说他们训练这个数据集

270
00:08:51,133 --> 00:08:54,233
就不是非常简单的Imagenet1k了

271
00:08:54,233 --> 00:08:55,933
可以包括更复杂一些数据集

272
00:08:55,933 --> 00:08:57,333
比如说Coyo啊

273
00:08:57,333 --> 00:08:59,666
包括一些非常大的一个Panda-70M

274
00:08:59,700 --> 00:09:02,333
这个应该是一个video dataset

275
00:09:02,366 --> 00:09:03,200
我们就直接看

276
00:09:03,200 --> 00:09:04,800
作者在table里面

277
00:09:04,800 --> 00:09:06,533
show的一个主要的结果吧

278
00:09:07,500 --> 00:09:09,166
其实作者想说的就是说

279
00:09:09,166 --> 00:09:11,400
在不同的这个DiT的模型

280
00:09:11,400 --> 00:09:13,133
参数上面对比较的话

281
00:09:13,133 --> 00:09:13,800
他们的方法会比得到这个FID

282
00:09:15,400 --> 00:09:17,500
包括FDD， FDD的话

283
00:09:17,500 --> 00:09:18,833
这个跟FID比起来的话

284
00:09:19,000 --> 00:09:19,433
这个Inception的这个neural network

285
00:09:21,200 --> 00:09:22,766
把它换成了DINO, DINO是 self-supervised learning

286
00:09:24,166 --> 00:09:24,966
的一个方法

287
00:09:25,600 --> 00:09:26,466
可以发现的话

288
00:09:26,466 --> 00:09:27,366
作者自己

289
00:09:27,366 --> 00:09:27,733
会比他们的baseline的这个FID

290
00:09:29,500 --> 00:09:30,466
都会好一些

291
00:09:30,933 --> 00:09:33,900
第二个这个加FT的话应该是指fine-tune

292
00:09:33,900 --> 00:09:35,900
作者主要是用这个fine-tune

293
00:09:35,900 --> 00:09:37,033
就是来做一个标准的一个对照实验

294
00:09:38,466 --> 00:09:40,366
好啦，关于这个idea的话

295
00:09:40,666 --> 00:09:42,066
我就通过这两篇paper

296
00:09:42,066 --> 00:09:44,000
来给大家介绍到这里啦

